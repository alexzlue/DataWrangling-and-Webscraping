{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Reddit\n",
    "## Sentiment Analysis in Product Subreddits\n",
    "\n",
    "### Packages used:\n",
    "* PRAW (Python Reddit API Wrapper)\n",
    "* VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "* nltk\n",
    "\n",
    "### Links used:\n",
    "* [PRAW tutorial](https://towardsdatascience.com/scraping-reddit-with-praw-76efc1d1e1d9)\n",
    "* [PRAW doc](https://praw.readthedocs.io/en/latest/index.html)\n",
    "* [VADER github](https://github.com/cjhutto/vaderSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instance of praw. All credetials stored in\n",
    "# praw.ini file \n",
    "reddit = praw.Reddit()\n",
    "\n",
    "print(reddit.user.me())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instances for the subreddits to investigate\n",
    "# we will first investigate the sentiment of big phone/tablet manuf\n",
    "samsung = reddit.subreddit('samsung')\n",
    "apple = reddit.subreddit('iphone')\n",
    "google = reddit.subreddit('googlepixel')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top in the last year\n",
    "s_top = samsung.top(time_filter = 'year', limit = 1000)\n",
    "a_top = apple.top(time_filter = 'year', limit = 1000)\n",
    "g_top = google.top(time_filter = 'year', limit = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With PRAW, everything is saved into a submission datatype\n",
    "# containing: title, score (upvotes), dateTime, author, etc\n",
    "# lets collect all of that\n",
    "manuf = {\n",
    "    \"company\"      : [],\n",
    "    \"score\"        : [],\n",
    "    \"datetime\"     : [],\n",
    "    \"author\"       : [],\n",
    "    \"title\"        : [],\n",
    "    \"selftext\"     : [],\n",
    "    \"permalink\"    : []\n",
    "}\n",
    "\n",
    "# we will now add each post into our dictionary with an added company column\n",
    "gener_list = [s_top, a_top, g_top]\n",
    "cnt = 0\n",
    "for subreddit in gener_list:\n",
    "    if cnt==0:\n",
    "        name = 'samsung'\n",
    "    elif cnt==1:\n",
    "        name = 'apple'\n",
    "    else:\n",
    "        name = 'google'\n",
    "        \n",
    "    for submission in subreddit:\n",
    "        manuf['company'].append(name)\n",
    "        manuf[\"title\"].append(submission.title)\n",
    "        manuf[\"score\"].append(submission.score)\n",
    "        manuf[\"datetime\"].append(dt.utcfromtimestamp(submission.created_utc))\n",
    "        manuf[\"author\"].append(submission.author)\n",
    "        if submission.selftext == \"\":\n",
    "            manuf[\"selftext\"].append(\" \")\n",
    "        else:\n",
    "            manuf[\"selftext\"].append(submission.selftext)\n",
    "        manuf[\"permalink\"].append(\"https://www.reddit.com\" + submission.permalink)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    cnt += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuf_df = pd.DataFrame(manuf)\n",
    "manuf_df.to_csv(\"./three_companies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuf_df = pd.read_csv(\"./three_companies.csv\")\n",
    "\n",
    "# print(manuf_df['selftext'][0])\n",
    "\n",
    "#display head\n",
    "manuf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now can work with VADER to do sentiment analysis on each post and title\n",
    "# because subreddits dont always contain text body, we want to concat the title\n",
    "# and selftext together to analyze all text per post.\n",
    "title_text_zip = zip(manuf_df.title, manuf_df.selftext)\n",
    "title_text = []\n",
    "for ti, txt in title_text_zip:\n",
    "    title_text.append(ti + \" \"+ txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# import vader analyzer. This will conduct the sentiment analysis\n",
    "# analysis will return with a dictionary with a positive, negative, \n",
    "# neutral, and compound score\n",
    "vder = SentimentIntensityAnalyzer()\n",
    "resp = []\n",
    "for text in title_text:\n",
    "    analysis = vder.polarity_scores(text)\n",
    "    resp.append(analysis)\n",
    "\n",
    "for i in range(10):\n",
    "    print(resp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 4 identifiers that are returned:\n",
    "# neg(ative), neu(tral), pos(itive), and compound.\n",
    "# compound will produce the net sentiment of a post.\n",
    "# pos if compound >=.05, neg if compound <=-.05, neu othw\n",
    "\n",
    "# add both sentiment and compound score to df\n",
    "vader_anlys = {\n",
    "    \"sentiment\" : [],\n",
    "    \"compound\"  : []\n",
    "}\n",
    "\n",
    "for i in resp:\n",
    "    if i['compound']<= -0.05:\n",
    "        vader_anlys[\"sentiment\"].append(\"negative\") \n",
    "        vader_anlys[\"compound\"].append(i['compound'])\n",
    "    elif i['compound']>= 0.05:\n",
    "        vader_anlys[\"sentiment\"].append(\"positive\") \n",
    "        vader_anlys[\"compound\"].append(i['compound'])\n",
    "    else:\n",
    "        vader_anlys[\"sentiment\"].append(\"neutral\") \n",
    "        vader_anlys[\"compound\"].append(i['compound'])\n",
    "        \n",
    "        \n",
    "manuf_df[\"sentiment\"] = vader_anlys[\"sentiment\"]\n",
    "manuf_df[\"compound\"] = vader_anlys[\"compound\"]\n",
    "\n",
    "manuf_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the pos/neg/neu sentiment grouped by phon manufacturer\n",
    "manuf_sentiment = manuf_df.groupby(['company', 'sentiment']).sentiment.count().unstack()  \n",
    "manuf_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuf_sentiment.plot(kind='bar')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can now use this to do a specific \n",
    "# analysis on a subreddit\n",
    "\n",
    "# Lets analyze DirecTVNow and use top, hot and\n",
    "# controversial posts of the last year for more data points\n",
    "dtvnow = reddit.subreddit('DirecTVNow')\n",
    "dtv_top = dtvnow.top(time_filter = 'year', limit = 1000)\n",
    "dtv_cont = dtvnow.controversial(time_filter = 'year', limit = 1000)\n",
    "dtv_hot = dtvnow.hot(limit = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With PRAW, everything is saved into a submission datatype\n",
    "# containing: title, score (upvotes), dateTime, author, etc\n",
    "# lets collect all of that\n",
    "dtvdata = {\n",
    "    \"score\"        : [],\n",
    "    \"datetime\"     : [],\n",
    "    \"author\"       : [],\n",
    "    \"title\"        : [],\n",
    "    \"selftext\"     : [],\n",
    "    \"permalink\"    : []\n",
    "}\n",
    "id_set = set()\n",
    "dup_id = []\n",
    "\n",
    "for filt in [dtv_top, dtv_cont, dtv_hot]:\n",
    "    time.sleep(5)\n",
    "    for submission in filt:\n",
    "        if submission.id not in id_set:\n",
    "            id_set.add(submission.id)\n",
    "            dtvdata[\"title\"].append(submission.title)\n",
    "            dtvdata[\"score\"].append(submission.score)\n",
    "            dtvdata[\"datetime\"].append(dt.utcfromtimestamp(submission.created_utc))\n",
    "            dtvdata[\"author\"].append(submission.author)\n",
    "            if submission.selftext == \"\":\n",
    "                dtvdata[\"selftext\"].append(\" \")\n",
    "            else:\n",
    "                dtvdata[\"selftext\"].append(submission.selftext)\n",
    "            dtvdata[\"permalink\"].append(\"https://www.reddit.com\" + submission.permalink)\n",
    "        else:\n",
    "            dup_id.append(submission.id)\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as a pandas df\n",
    "dtv_df = pd.DataFrame(dtvdata)\n",
    "\n",
    "# for key in dtvdata.keys():\n",
    "#     print(key, len(dtvdata[key]))\n",
    "print(len(dup_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtv_df\n",
    "# move to a csv\n",
    "dtv_df.to_csv(\"./dtvnow.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtv_df = pd.read_csv(\"./dtvnow.csv\")\n",
    "\n",
    "# Display some of the results we got\n",
    "\n",
    "dtv_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now can work with VADER to do sentiment analysis on each post and title\n",
    "# because subreddits dont always contain text body, we want to concat the title\n",
    "# and selftext together to analyze all text per post.\n",
    "title_text_zip = zip(dtv_df.title, dtv_df.selftext)\n",
    "title_text = []\n",
    "for ti, txt in title_text_zip:\n",
    "    title_text.append(ti + \" \"+ txt)\n",
    "\n",
    "# title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# import vader analyzer. This will conduct the sentiment analysis\n",
    "vder = SentimentIntensityAnalyzer()\n",
    "resp = []\n",
    "for text in title_text:\n",
    "    analysis = vder.polarity_scores(text)\n",
    "    resp.append(analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort VADER sentiment\n",
    "\n",
    "# add both sentiment and compound score to df\n",
    "vader_anlys = {\n",
    "    \"sentiment\" : [],\n",
    "    \"compound\"  : [],\n",
    "}\n",
    "\n",
    "for i in resp:\n",
    "    if i['compound']<= -0.05:\n",
    "        vader_anlys[\"sentiment\"].append(\"negative\") \n",
    "        vader_anlys[\"compound\"].append(i['compound'])\n",
    "    elif i['compound']>= 0.05:\n",
    "        vader_anlys[\"sentiment\"].append(\"positive\") \n",
    "        vader_anlys[\"compound\"].append(i['compound'])\n",
    "    else:\n",
    "        vader_anlys[\"sentiment\"].append(\"neutral\") \n",
    "        vader_anlys[\"compound\"].append(i['compound'])\n",
    "        \n",
    "        \n",
    "dtv_df[\"sentiment\"] = vader_anlys[\"sentiment\"]\n",
    "dtv_df[\"compound\"] = vader_anlys[\"compound\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtv_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to a csv\n",
    "\n",
    "dtv_df.to_csv(\"./dtvnow_with_sentiment.csv\", index=False)\n",
    "dtv_df.loc[dtv_df['sentiment'] == 'negative'].to_csv(\"./dtvnow_negative_sentiment.csv\", index=False)\n",
    "dtv_df.loc[dtv_df['sentiment'] == 'positive'].to_csv(\"./dtvnow_positive_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(dtv_df['sentiment'].value_counts())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "dtv_df['sentiment'].value_counts().plot(ax=ax, kind='pie', autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now take a look at the positives and negative\n",
    "# posts and see if we can find any specific key words\n",
    "\n",
    "# we can use nltk for this\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "\n",
    "# DataFrames containing just pos/neg sentiment posts\n",
    "neg_df = dtv_df.loc[dtv_df['sentiment'] == 'negative']\n",
    "pos_df = dtv_df.loc[dtv_df['sentiment'] == 'positive']\n",
    "\n",
    "# def to concat title and text\n",
    "def title_text(zip_gen):\n",
    "    zip_arr = []\n",
    "    for ti, tx in zip_gen:\n",
    "        zip_arr.append(ti + \" \" + tx)\n",
    "    \n",
    "    return zip_arr\n",
    "\n",
    "neg_df_zip = zip(neg_df.title, neg_df.selftext)\n",
    "pos_df_zip = zip(pos_df.title, pos_df.selftext)\n",
    "\n",
    "neg_txt = title_text(neg_df_zip)\n",
    "pos_txt = title_text(pos_df_zip)\n",
    "\n",
    "print(neg_txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk to remove stop words to increase frequency clarity\n",
    "# put into fcn\n",
    "def remove_stop_wds(text_list):\n",
    "    # regex found here: https://gist.github.com/ameyavilankar/10347201\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in range(len(text_list)):\n",
    "        text = text_list[i].lower()\n",
    "        word_tokens = tokenizer.tokenize(text)\n",
    "        filtered = [w for w in word_tokens if not w in stop_words]\n",
    "        text_list[i] = filtered\n",
    "    \n",
    "remove_stop_wds(neg_txt)\n",
    "remove_stop_wds(pos_txt)\n",
    "\n",
    "print(neg_txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a function that displays the top 10 \n",
    "# most common words, and output to a pandas df\n",
    "def word_freq_df(text_list):\n",
    "    frq = {}\n",
    "    for txt in text_list:\n",
    "        word_frequency = nltk.FreqDist(txt)\n",
    "        for word, frequency in word_frequency.most_common():\n",
    "            if word not in frq:\n",
    "                frq[word] = frequency\n",
    "            else:\n",
    "                frq[word] += frequency\n",
    "    w_f = {\n",
    "        'word'      : [],\n",
    "        'frequency' : []\n",
    "    }\n",
    "    for w in frq.keys():\n",
    "        w_f['word'].append(w)\n",
    "        w_f['frequency'].append(frq[w])\n",
    "    \n",
    "    frequency_df = pd.DataFrame(w_f).sort_values(by=['frequency'], ascending = False)\n",
    "    return frequency_df\n",
    "    \n",
    "neg_df = word_freq_df(neg_txt)\n",
    "pos_df = word_freq_df(pos_txt)\n",
    "# neg_df\n",
    "# pos_df.head(100)\n",
    "\n",
    "ngx = neg_df[:20].plot.bar(x='word',y='frequency', legend=False, rot = 50, title='Negative Sentiment Word Frequency')\n",
    "ngx.set_ylabel('frequency')\n",
    "psx = pos_df[:20].plot.bar(x='word',y='frequency', legend=False, rot = 50, title='Positive Sentiment Word Frequency')\n",
    "psx.set_ylabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to train our own model with our data now.\n",
    "# So we can use a 80-20 split of our pos/neg data\n",
    "# to create a svm.\n",
    "\n",
    "#first clean and prepare data\n",
    "# print(int(2489*0.75))\n",
    "totals = dtv_df['sentiment'].value_counts()\n",
    "# print(totals)\n",
    "\n",
    "dtv_df['titxt'] = dtv_df['title'] + \" \" + dtv_df['selftext']\n",
    "\n",
    "# randomize our data.\n",
    "# did not want to use train_test_split since i wanted to\n",
    "# keep an 80/20 split of pos and neg posts too\n",
    "dtv_df = dtv_df.sample(frac = 1.0)\n",
    "\n",
    "postrain = dtv_df.loc[dtv_df['sentiment'] == 'positive'][:int(totals['positive']*0.80)]\n",
    "postest = dtv_df.loc[dtv_df['sentiment'] == 'positive'][int(totals['positive']*0.80):]\n",
    "negtrain = dtv_df.loc[dtv_df['sentiment'] == 'negative'][:int(totals['negative']*0.80)]\n",
    "negtest = dtv_df.loc[dtv_df['sentiment'] == 'negative'][int(totals['negative']*0.80):]\n",
    "# print(len(postrain), len(postest))\n",
    "# print(len(negtrain), len(negtest))\n",
    "train = pd.concat([postrain,negtrain])\n",
    "test = pd.concat([postest,negtest])\n",
    "# print(len(train),len(test))\n",
    "\n",
    "dtv_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create feature vectors using a count vectorizer\n",
    "# forms feature vectors based off groupings of\n",
    "# ngram words removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(binary=True, ngram_range=(1, 3), \n",
    "                             stop_words=stop_words)\n",
    "\n",
    "# a tf-idf transformer will create feature vectors\n",
    "# based off term frequency but will weight the frequency \n",
    "# of terms in a document less to lessen bias/skew\n",
    "\n",
    "\n",
    "#creating vectors for my test and train data\n",
    "train_vectors = vectorizer.fit_transform(train['titxt'])\n",
    "test_vectors = vectorizer.transform(test['titxt'])\n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "# varying c levels dependent on label noise\n",
    "for c in [1, 1.01, 1.05, 1.1, 1.15, 1.5, 2, 5, 10]:\n",
    "    classifier_linear = svm.SVC(kernel='linear', C = c)\n",
    "    t0 = time.time()\n",
    "    classifier_linear.fit(train_vectors, train['sentiment'])\n",
    "    t1 = time.time()\n",
    "    prediction_linear = classifier_linear.predict(test_vectors)\n",
    "    t2 = time.time()\n",
    "    time_linear_train = t1-t0\n",
    "    time_linear_predict = t2-t1\n",
    "\n",
    "    # results\n",
    "    print(\"Training time: \" + str(time_linear_train) + \"s; Prediction time: \"+ str(time_linear_predict) +\"s; C: \"+ str(c))\n",
    "\n",
    "    report = classification_report(test['sentiment'], prediction_linear, output_dict=True)\n",
    "    print('positive: ', report['positive'])\n",
    "    print('negative: ', report['negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
